{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/tien/Documents/PythonEnvs/pytorch/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "# import zipfile\n",
    "#from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "#from collections import defaultdict\n",
    "#from urllib.error import URLError\n",
    "#from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F \n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from jup.utils import set_random_seed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jup.recsys_models.data.movielens_1m import read_data\n",
    "from jup.recsys_models.data.movielens_1m import create_dataset\n",
    "from jup.recsys_models.data.movielens_1m import tabular_preview\n",
    "from jup.recsys_models.data.movielens_1m import RatingsIterator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jup.utils import learning_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, movies = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title                        genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>110</th>\n",
       "      <th>260</th>\n",
       "      <th>480</th>\n",
       "      <th>589</th>\n",
       "      <th>593</th>\n",
       "      <th>608</th>\n",
       "      <th>1196</th>\n",
       "      <th>1198</th>\n",
       "      <th>1210</th>\n",
       "      <th>1270</th>\n",
       "      <th>1580</th>\n",
       "      <th>2028</th>\n",
       "      <th>2571</th>\n",
       "      <th>2762</th>\n",
       "      <th>2858</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4344</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  110   260   480   589   593   608   1196  1198  1210  1270  1580  \\\n",
       "userId                                                                      \n",
       "889       4.0   4.0   3.0   5.0   5.0   4.0   4.0   NaN   3.0   4.0   3.0   \n",
       "1015      4.0   5.0   4.0   5.0   5.0   5.0   4.0   5.0   4.0   4.0   4.0   \n",
       "1150      2.0   5.0   NaN   2.0   3.0   5.0   4.0   2.0   3.0   2.0   2.0   \n",
       "1181      3.0   4.0   2.0   5.0   3.0   3.0   4.0   3.0   3.0   3.0   4.0   \n",
       "1449      3.0   3.0   2.0   2.0   5.0   5.0   3.0   4.0   2.0   2.0   4.0   \n",
       "1680      1.0   2.0   5.0   5.0   5.0   5.0   5.0   5.0   3.0   3.0   4.0   \n",
       "1941      5.0   5.0   5.0   3.0   5.0   4.0   5.0   5.0   5.0   5.0   5.0   \n",
       "1980      4.0   4.0   4.0   4.0   5.0   5.0   4.0   5.0   4.0   5.0   4.0   \n",
       "2063      5.0   4.0   4.0   2.0   5.0   2.0   4.0   4.0   4.0   4.0   3.0   \n",
       "2909      5.0   5.0   5.0   4.0   5.0   5.0   5.0   5.0   5.0   5.0   5.0   \n",
       "3618      3.0   4.0   4.0   3.0   5.0   4.0   3.0   4.0   3.0   4.0   3.0   \n",
       "4169      4.0   5.0   5.0   4.0   5.0   5.0   5.0   5.0   5.0   4.0   4.0   \n",
       "4277      5.0   5.0   5.0   5.0   5.0   5.0   5.0   5.0   4.0   5.0   4.0   \n",
       "4344      5.0   5.0   4.0   4.0   2.0   5.0   5.0   5.0   5.0   4.0   3.0   \n",
       "5795      5.0   5.0   5.0   4.0   5.0   4.0   5.0   5.0   4.0   5.0   1.0   \n",
       "\n",
       "movieId  2028  2571  2762  2858  \n",
       "userId                           \n",
       "889       3.0   5.0   NaN   2.0  \n",
       "1015      5.0   5.0   5.0   4.0  \n",
       "1150      2.0   1.0   2.0   4.0  \n",
       "1181      4.0   5.0   4.0   3.0  \n",
       "1449      3.0   4.0   4.0   4.0  \n",
       "1680      5.0   3.0   5.0   5.0  \n",
       "1941      5.0   3.0   5.0   1.0  \n",
       "1980      5.0   5.0   5.0   5.0  \n",
       "2063      2.0   5.0   4.0   5.0  \n",
       "2909      5.0   4.0   5.0   5.0  \n",
       "3618      3.0   3.0   4.0   4.0  \n",
       "4169      5.0   4.0   5.0   5.0  \n",
       "4277      5.0   5.0   5.0   5.0  \n",
       "4344      2.0   5.0   5.0   5.0  \n",
       "5795      5.0   1.0   2.0   5.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_preview(ratings, movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 6040 users, 3706 movies\n",
      "Dataset shape: (1000209, 2)\n",
      "Target shape: (1000209,)\n"
     ]
    }
   ],
   "source": [
    "(n, m), (X, y), _ = create_dataset(ratings)\n",
    "print(f'Embeddings: {n} users, {m} movies')\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(user_rating_matrix, ratings, batch_size=32, shuffle=True):\n",
    "    for xb, yb in RatingsIterator(user_movie_matrix=user_rating_matrix, ratings=ratings, batch_size=batch_size, shuffle=shuffle):\n",
    "        xb = torch.LongTensor(xb)\n",
    "        yb = torch.FloatTensor(yb)\n",
    "        \n",
    "        '''\n",
    "        FOr matrix factorization, we have the issue below.\n",
    "        Thus, we wont need to make a change in shape\n",
    "        /Users/tien/Documents/PythonEnvs/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([2000, 1])) that is different to the input size (torch.Size([2000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "        return F.mse_loss(input, target, reduction=self.reduction)\n",
    "        \n",
    "        '''\n",
    "        yield xb, yb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module is the base class for all the network\n",
    "\n",
    "class MF(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a dense network with embedding layers.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        n_users:            \n",
    "            Number of unique users in the dataset.\n",
    "\n",
    "        n_movies: \n",
    "            Number of unique movies in the dataset.\n",
    "\n",
    "        n_factors: \n",
    "            Number of columns in the embeddings matrix.     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_movies, n_factors=50):\n",
    "        \n",
    "        super().__init__() # because we subclass nn.Module\n",
    "        \n",
    "        # Build the matrix to store the embedding for n_users\n",
    "        # Embedding: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        # A simple look up table that stores embeddingof a fixed dictionary and size.\n",
    "        # n_users: n number of embeddings\n",
    "        self.u = nn.Embedding(n_users, n_factors)\n",
    "        \n",
    "        # Build the matrix to store the embedding for m_movies\n",
    "        # A  look up table that stores embeddings for n_movies\n",
    "        self.m = nn.Embedding(n_movies, n_factors)\n",
    "        \n",
    "        # Initialize the embedding vector\n",
    "        nn.init.normal_(self.u.weight, 0, 0.1)\n",
    "        nn.init.normal_(self.m.weight, 0, 0.1)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, users, movies): \n",
    "        users_latent = self.u(users)\n",
    "        movies_latent = self.m(movies)\n",
    "        \n",
    "        # Need to understand what dim = 1 here means (TODO)\n",
    "        # Need to understand what this does here\n",
    "        # Note:\n",
    "        # The passed in information is the pair (user, movie).\n",
    "        # Meaning: \n",
    "        #   users is a list of users\n",
    "        #   movies is a list of movies\n",
    "        #   The two lists have the same size\n",
    "        #   At a given index i, we compute the rating \n",
    "        #   of a user @ users[i] with regards to a movie @ movies[i]\n",
    "        # we do not do a matrix multiplication here as this is pair-wise computation\n",
    "        # For more understanding, see the note here:\n",
    "        # https://developers.google.com/machine-learning/recommendation/collaborative/matrix\n",
    "        # Note: Observe that the (i,j) entry of U . V (transpose) of the embeddings of\n",
    "        # a user i and item j\n",
    "        # \n",
    "        # Below is the dot product of the two embeddinds with the same len\n",
    "        # we sum() so that each user & movie pair only have one value (ratin)\n",
    "        return (users_latent * movies_latent).sum(dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF(\n",
    "    n_users=n,\n",
    "    n_movies=m,\n",
    "    n_factors=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MF(\n",
       "  (u): Embedding(6040, 50)\n",
       "  (m): Embedding(3706, 50)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "datasets = {'train': (X_train, y_train), 'val': (X_valid, y_valid)}\n",
    "dataset_sizes = {'train': len(X_train), 'val': len(X_valid)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529184</th>\n",
       "      <td>3270</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341591</th>\n",
       "      <td>2011</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470922</th>\n",
       "      <td>2898</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630004</th>\n",
       "      <td>3807</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131938</th>\n",
       "      <td>853</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491263</th>\n",
       "      <td>3019</td>\n",
       "      <td>1158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791624</th>\n",
       "      <td>4731</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470924</th>\n",
       "      <td>2898</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491755</th>\n",
       "      <td>3024</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128037</th>\n",
       "      <td>827</td>\n",
       "      <td>1363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800167 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  movie_id\n",
       "529184     3270      1925\n",
       "341591     2011       579\n",
       "470922     2898      1497\n",
       "630004     3807       737\n",
       "131938      853       536\n",
       "...         ...       ...\n",
       "491263     3019      1158\n",
       "791624     4731       560\n",
       "470924     2898       964\n",
       "491755     3024       411\n",
       "128037      827      1363\n",
       "\n",
       "[800167 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss improvement on epoch: 1\n",
      "[001/100] train: 14.0169 - val: 13.7831\n",
      "loss improvement on epoch: 2\n",
      "[002/100] train: 13.0903 - val: 12.6661\n",
      "loss improvement on epoch: 3\n",
      "[003/100] train: 6.8024 - val: 2.5218\n",
      "loss improvement on epoch: 4\n",
      "[004/100] train: 1.8740 - val: 1.7005\n",
      "loss improvement on epoch: 5\n",
      "[005/100] train: 1.1976 - val: 1.0280\n",
      "loss improvement on epoch: 6\n",
      "[006/100] train: 0.9423 - val: 0.9661\n",
      "loss improvement on epoch: 7\n",
      "[007/100] train: 0.8754 - val: 0.8760\n",
      "loss improvement on epoch: 8\n",
      "[008/100] train: 0.8218 - val: 0.8616\n",
      "loss improvement on epoch: 9\n",
      "[009/100] train: 0.8078 - val: 0.8354\n",
      "loss improvement on epoch: 10\n",
      "[010/100] train: 0.7768 - val: 0.8279\n",
      "loss improvement on epoch: 11\n",
      "[011/100] train: 0.7701 - val: 0.8134\n",
      "loss improvement on epoch: 12\n",
      "[012/100] train: 0.7414 - val: 0.8070\n",
      "loss improvement on epoch: 13\n",
      "[013/100] train: 0.7367 - val: 0.7964\n",
      "loss improvement on epoch: 14\n",
      "[014/100] train: 0.7090 - val: 0.7915\n",
      "loss improvement on epoch: 15\n",
      "[015/100] train: 0.7057 - val: 0.7847\n",
      "loss improvement on epoch: 16\n",
      "[016/100] train: 0.6787 - val: 0.7806\n",
      "loss improvement on epoch: 17\n",
      "[017/100] train: 0.6766 - val: 0.7762\n",
      "loss improvement on epoch: 18\n",
      "[018/100] train: 0.6501 - val: 0.7727\n",
      "loss improvement on epoch: 19\n",
      "[019/100] train: 0.6487 - val: 0.7698\n",
      "loss improvement on epoch: 20\n",
      "[020/100] train: 0.6230 - val: 0.7671\n",
      "loss improvement on epoch: 21\n",
      "[021/100] train: 0.6228 - val: 0.7663\n",
      "loss improvement on epoch: 22\n",
      "[022/100] train: 0.5978 - val: 0.7644\n",
      "[023/100] train: 0.5986 - val: 0.7651\n",
      "loss improvement on epoch: 24\n",
      "[024/100] train: 0.5745 - val: 0.7639\n",
      "[025/100] train: 0.5762 - val: 0.7660\n",
      "[026/100] train: 0.5530 - val: 0.7650\n",
      "[027/100] train: 0.5557 - val: 0.7687\n",
      "[028/100] train: 0.5331 - val: 0.7680\n",
      "[029/100] train: 0.5365 - val: 0.7725\n",
      "[030/100] train: 0.5148 - val: 0.7720\n",
      "[031/100] train: 0.5188 - val: 0.7774\n",
      "[032/100] train: 0.4978 - val: 0.7770\n",
      "[033/100] train: 0.5024 - val: 0.7828\n",
      "[034/100] train: 0.4820 - val: 0.7828\n",
      "early stopping after epoch 034\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5  # weight decay (TODO)\n",
    "bs = 2000  # batch size\n",
    "n_epochs = 100\n",
    "patience = 10\n",
    "no_improvements = 0\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')  # criterion\n",
    "\n",
    "'''\n",
    "Adaptive Moment Estimation (Adam) is another method that computes adaptive \n",
    "learning rates for each parameter. In addition to storing an exponentially \n",
    "decaying average of past squared gradients, \n",
    "Adam also keeps an exponentially decaying average of past gradients v, \n",
    "similar to momentum. Whereas momentum can be seen as a ball running down a slope, \n",
    "Adam behaves like a heavy ball with friction,  \n",
    "which thus prefers flat minima in the error surface.\n",
    "'''\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)  # TODO\n",
    "iterations_per_epoch = int(math.ceil(dataset_sizes['train'] // bs))\n",
    "scheduler = learning_rate.CyclicLR(optimizer, learning_rate.cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))\n",
    "\n",
    "writer = SummaryWriter('runs/mf')\n",
    "\n",
    "# Maybe train just on one epoch\n",
    "add_model_to_tensorboard = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "          training = True\n",
    "        else:\n",
    "          training = False\n",
    "\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in batches(*datasets[phase], shuffle=training, batch_size=bs):\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            \n",
    "            # learn up the gradient after each batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "          \n",
    "            if not add_model_to_tensorboard:\n",
    "                writer.add_graph(model, [x_batch[:,0], x_batch[:,1]])\n",
    "                # writer.close()\n",
    "                add_model_to_tensorboard = True \n",
    "                \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                # x_batch[:,0]: take everything in the first (0-index row): users\n",
    "                # y_batch[:,1]: take everything in the first (1-index row): movies\n",
    "                \n",
    "                # call the forward method\n",
    "                # only take in the available user movie pairs here\n",
    "                # not a whole user x movie pair\n",
    "                # tha can be explain why \n",
    "                outputs = model(x_batch[:,0], x_batch[:,1])\n",
    "                preds = torch.round(outputs)  # look into this\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "\n",
    "                    # loss.backward() computes dloss/dx for every parameter \n",
    "                    # x which has requires_grad=True. \n",
    "                    # These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
    "                    # x.grad += dloss/dx\n",
    "                    # source: https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # optimizer.step updates the value of x using the gradient x.grad. \n",
    "                    # For example, the SGD optimizer performs:\n",
    "                    # x += -lr * x.grad\n",
    "                    # optimizer.zero_grad() -> should not put it here as the eval value is really bad\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # we call it so that the learning rate will change after each epoch\n",
    "                    # https://discuss.pytorch.org/t/what-does-scheduler-step-do/47764\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    # relationship between loss.backward() and optimizer.step()\n",
    "                    # https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step\n",
    "                    \n",
    "                    # This is just to save the lr rate to lr_history\n",
    "                    # so that we can plot later.\n",
    "                    lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # If we have a longer epoch, then we can just add the loss every 100 epoch or so\n",
    "            # In this case, the number of epoch is less, so we can just add that in here.\n",
    "            writer.add_scalar('training_loss', running_loss, global_step=epoch)\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break\n",
    "    \n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34702dd390fd4fc9cf54421ef6e48a33a0e01434706512affc37171929b3d3dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
